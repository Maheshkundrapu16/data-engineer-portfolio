{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96653382-6493-4ccc-84f3-7e8607e8665b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#test pubic data set read\n",
    "df = spark.read.csv('/databricks-datasets/COVID/covid-19-data/us-counties.csv', header=True, inferSchema=True)\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085b4a55-ba50-4aeb-81b9-ec3371a6d5fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Ingest Public COVID-19 Data on Databricks\n",
    "covid_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/databricks-datasets/COVID/covid-19-data/us-counties.csv\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa04016e-b35b-46cf-aa0d-032f1e87962a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning - Remove nulls and filter for relevant timeframe\n",
    "covid_clean = (\n",
    "    covid_df\n",
    "    .dropna(subset=[\"cases\", \"deaths\", \"county\", \"state\"])\n",
    "    .filter(\"date >= '2021-01-01' AND date <= '2021-12-31'\")\n",
    ")\n",
    "\n",
    "# Step 3: Feature Engineering - Calculate daily new cases and deaths for each county/state\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window_spec = Window.partitionBy(\"county\", \"state\").orderBy(\"date\")\n",
    "covid_features = (\n",
    "    covid_clean\n",
    "    .withColumn(\"new_cases\", F.col(\"cases\") - F.lag(\"cases\", 1).over(window_spec))\n",
    "    .withColumn(\"new_deaths\", F.col(\"deaths\") - F.lag(\"deaths\", 1).over(window_spec))\n",
    "    .fillna({\"new_cases\": 0, \"new_deaths\": 0})\n",
    ")\n",
    "\n",
    "# Step 4: Aggregation - Compute total new cases by state\n",
    "state_agg = (\n",
    "    covid_features\n",
    "    .groupBy(\"state\")\n",
    "    .agg(\n",
    "        F.sum(\"new_cases\").alias(\"total_new_cases\"),\n",
    "        F.sum(\"new_deaths\").alias(\"total_new_deaths\"),\n",
    "        F.countDistinct(\"county\").alias(\"distinct_counties\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_new_cases\"))\n",
    ")\n",
    "\n",
    "# Step 5: Visualization - Display top 10 states by cases\n",
    "display(state_agg.limit(10))\n",
    "\n",
    "# Step 6: Save Results To Data Lake/Delta Table (optional for full pipeline)\n",
    "# state_agg.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/silver/covid_state_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2234d2-9f5d-4c04-90a6-3cbfc2e22acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Calculate 7-day moving average of new cases for each county\n",
    "window_7day = Window.partitionBy(\"county\", \"state\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "covid_features = covid_features.withColumn(\n",
    "    \"new_cases_7day_avg\",\n",
    "    F.avg(\"new_cases\").over(window_7day)\n",
    ")\n",
    "\n",
    "# 2. Flag counties with case surges (new_cases > 150% of 7-day average)\n",
    "covid_features = covid_features.withColumn(\n",
    "    \"is_surge\",\n",
    "    F.when(F.col(\"new_cases\") > 1.5 * F.col(\"new_cases_7day_avg\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 3. Add month, week, day features from the date\n",
    "covid_features = (\n",
    "    covid_features\n",
    "    .withColumn(\"year\", F.year(\"date\"))\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .withColumn(\"week\", F.weekofyear(\"date\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    ")\n",
    "\n",
    "# 4. Create cumulative cases/deaths by county for dimension modeling\n",
    "window_cum = Window.partitionBy(\"county\", \"state\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "covid_features = covid_features.withColumn(\n",
    "    \"cum_cases_county\",\n",
    "    F.sum(\"new_cases\").over(window_cum)\n",
    ").withColumn(\n",
    "    \"cum_deaths_county\",\n",
    "    F.sum(\"new_deaths\").over(window_cum)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfe3e61-fd26-4144-992a-6457fef61b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Read COVID-19 case data (already done in previous steps)\n",
    "covid_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/databricks-datasets/COVID/covid-19-data/us-counties.csv\")\n",
    ")\n",
    "\n",
    "# Step 2: Clean data and filter for 2021\n",
    "covid_clean = (\n",
    "    covid_df\n",
    "    .dropna(subset=[\"cases\", \"deaths\", \"county\", \"state\"])\n",
    "    .filter(\"date >= '2021-01-01' AND date <= '2021-12-31'\")\n",
    ")\n",
    "\n",
    "# Step 3: Calculate daily new cases and deaths using window function\n",
    "window_spec = Window.partitionBy(\"county\", \"state\").orderBy(\"date\")\n",
    "covid_features = (\n",
    "    covid_clean\n",
    "    .withColumn(\"new_cases\", F.col(\"cases\") - F.lag(\"cases\", 1).over(window_spec))\n",
    "    .withColumn(\"new_deaths\", F.col(\"deaths\") - F.lag(\"deaths\", 1).over(window_spec))\n",
    "    .fillna({\"new_cases\": 0, \"new_deaths\": 0})\n",
    ")\n",
    "\n",
    "# Step 4: Read US county population dataset from public CSV uploaded to DBFS\n",
    "# Feel free to replace this path with your own uploaded file location if needed\n",
    "pop_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/Volumes/workspace/default/test/uscounties.csv\")\n",
    ")\n",
    "pop_df = pop_df.withColumnRenamed(\"county\", \"County_US\")\n",
    "# Step 5: Join population data on county and state (matching column names assumed county and state_name)\n",
    "covid_enriched = covid_features.join(\n",
    "    pop_df,\n",
    "    (F.lower(covid_features.county) == F.lower(pop_df.County_US)) & (F.lower(covid_features.state) == F.lower(pop_df.state_name)),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Step 6: Calculate per-capita cases and deaths per 10k population\n",
    "covid_enriched = covid_enriched.withColumn(\n",
    "    \"cases_per_10k\",\n",
    "    F.round((F.col(\"cases\") / F.col(\"population\")) * 10000, 2)\n",
    ").withColumn(\n",
    "    \"deaths_per_10k\",\n",
    "    F.round((F.col(\"deaths\") / F.col(\"population\")) * 10000, 2)\n",
    ")\n",
    "\n",
    "# Step 7: Additional transformations - 7-day new case moving average and surge detection\n",
    "window_7day = Window.partitionBy(\"county\", \"state\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "covid_enriched = covid_enriched.withColumn(\n",
    "    \"new_cases_7day_avg\",\n",
    "    F.avg(\"new_cases\").over(window_7day)\n",
    ").withColumn(\n",
    "    \"is_surge\",\n",
    "    F.when(F.col(\"new_cases\") > 1.5 * F.col(\"new_cases_7day_avg\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Step 8: Show top 10 counties by total cases\n",
    "from pyspark.sql.functions import desc\n",
    "covid_enriched.groupBy(\"county\", \"state\")\\\n",
    "    .agg(F.max(\"cases\").alias(\"max_cases\"), F.max(\"deaths\").alias(\"max_deaths\"))\\\n",
    "    .orderBy(desc(\"max_cases\"))\\\n",
    "    .limit(10)\\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0119eb-8da8-4b67-b55f-244155fd1e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Previous steps assumed covid_enriched dataframe is ready from earlier code\n",
    "\n",
    "# --- Change Point Detection Simulation ---\n",
    "# Using 7-day rolling average delta to flag potential change points\n",
    "window_7day = Window.partitionBy(\"county\", \"state\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "window_prev = Window.partitionBy(\"county\", \"state\").orderBy(\"date\")\n",
    "covid_enriched = covid_enriched.withColumn(\n",
    "    \"new_cases_7day_avg\",\n",
    "    F.avg(\"new_cases\").over(window_7day)\n",
    ")\n",
    "covid_enriched = covid_enriched.withColumn(\n",
    "    \"prev_7day_avg\",\n",
    "    F.lag(\"new_cases_7day_avg\", 1).over(window_prev)\n",
    ")\n",
    "covid_enriched = covid_enriched.withColumn(\n",
    "    \"change_point_flag\",\n",
    "    F.when(F.abs(F.col(\"new_cases_7day_avg\") - F.col(\"prev_7day_avg\")) > 50, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# --- SCD Type 2 Dimension Table for Counties ---\n",
    "# Adding versioning and effective date columns for slowly changing dimension\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "dim_county = (\n",
    "    covid_enriched.select(\"county\", \"state\")\n",
    "    .dropDuplicates()\n",
    "    .withColumn(\"scd_version\", F.lit(1))\n",
    "    .withColumn(\"effective_date\", current_date())\n",
    ")\n",
    "\n",
    "# --- Week-over-week percentage change in new_cases ---\n",
    "covid_enriched = covid_enriched.withColumn(\"week\", F.weekofyear(\"date\"))\n",
    "window_week = Window.partitionBy(\"county\", \"state\", \"week\").orderBy(\"date\")\n",
    "weekly_sum = (\n",
    "    covid_enriched.groupBy(\"county\", \"state\", \"week\")\n",
    "    .agg(F.sum(\"new_cases\").alias(\"weekly_cases\"))\n",
    ")\n",
    "weekly_change = (\n",
    "    weekly_sum.withColumn(\n",
    "        \"prev_week_cases\",\n",
    "        F.lag(\"weekly_cases\", 1).over(Window.partitionBy(\"county\", \"state\").orderBy(\"week\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"wo_w_change_pct\",\n",
    "        F.when(\n",
    "            F.col(\"prev_week_cases\").isNotNull(),\n",
    "            (F.col(\"weekly_cases\") - F.col(\"prev_week_cases\")) / F.col(\"prev_week_cases\") * 100\n",
    "        ).otherwise(0)\n",
    "    )\n",
    ")\n",
    "# Join weekly percent changes back to main table by county, state, week\n",
    "covid_enriched = covid_enriched.join(\n",
    "    weekly_change.select(\"county\", \"state\", \"week\", \"wo_w_change_pct\"),\n",
    "    on=[\"county\", \"state\", \"week\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf42ef4-0fba-4c5f-9416-8ae75cb434b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- County Surge Frequency and Ranking ---\n",
    "county_surge_freq = (\n",
    "    covid_enriched.groupBy(\"county\", \"state\")\n",
    "    .agg(F.sum(\"is_surge\").alias(\"surge_count\"))\n",
    "    .orderBy(F.desc(\"surge_count\"))\n",
    ")\n",
    "display(county_surge_freq.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "294b8f4a-b087-4e0a-a6b7-abc350280a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "experiments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
